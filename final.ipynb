{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext\n",
    "print(torchtext.__version__) # 0.3.1\n",
    "from torchtext import data\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "import json\n",
    "import re\n",
    "\n",
    "from rouge import Rouge\n",
    "from inltk.inltk import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! git clone https://github.com/akashe/Python-Code-Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./Code/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing import getTokenizer,getData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -c \"http://www.phontron.com/download/conala-corpus-v1.1.zip\"\n",
    "# from inltk.inltk import setup\n",
    "# setup('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !head -20 conala-corpus/conala-test.json\n",
    "# with open('./conala-corpus/conala-test.json') as f:\n",
    "#     for _ in range(20): # first 10 lines\n",
    "#         print(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions, answers = [],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loading train json\n",
    "# f = open(\"./conala-corpus/conala-train.json\",\"r\")\n",
    "# train_file = json.load(f)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for num,i in enumerate(train_file):\n",
    "#     if i['intent'] is not None:\n",
    "#         questions.append(i['intent'])\n",
    "#         answers.append(i['snippet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loading test json\n",
    "# f = open(\"./conala-corpus/conala-test.json\",\"r\")\n",
    "# test_file = json.load(f)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for num,i in enumerate(test_file):\n",
    "#     if i['intent'] is not None:\n",
    "#         questions.append(i['intent'])\n",
    "#         answers.append(i['snippet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions[50:70],answers[50:70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions_, answers_ = getData(\"./Python-Code-Generation/data/english_python_data_pruned.txt\")\n",
    "questions_, answers_ = getData('./Data/Data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting max word len\n",
    "max_word_len = 301"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in tokenization\n",
      "Error in tokenization\n",
      "Error in tokenization\n",
      "Error in tokenization\n",
      "Error in tokenization\n",
      "1058\n"
     ]
    }
   ],
   "source": [
    "# removing examples with len more than max_word_len\n",
    "# Only checking in original data because I checked in CONALA data and answers their dont exceed 301.\n",
    "pruned_questions = []\n",
    "pruned_answers = []\n",
    "for j,i in zip(questions_,answers_):\n",
    "    tokens = getTokenizer(i)\n",
    "    if not len(tokens) > max_word_len:\n",
    "        pruned_answers.append(i)\n",
    "        pruned_questions.append(j) \n",
    "\n",
    "print(len(pruned_answers))\n",
    "answers_ = pruned_answers\n",
    "questions_ = pruned_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = questions + questions_\n",
    "answers = answers + answers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('संख्या पूरी तरह से विभाजित होने पर तोड़ने के लिए एक पायथन प्रोग्राम लिखें',\n",
       " 'i = 1\\nwhile True:\\n\\tif i%3 == 0:\\n\\t\\tbreak\\n\\tprint(i)\\n\\ti+= 1\\n')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[100],answers[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1058 1058\n"
     ]
    }
   ],
   "source": [
    "print(len(questions),len(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1327\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy_en = spacy.load('en_core_web_sm')\n",
    "# tok_hin = tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_hi(text):\n",
    "    \"\"\"\n",
    "    Tokenizes Hindi text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok for tok in tokenize(text, 'hi')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Field(tokenize = tokenize_hi, \n",
    "            init_token = '', \n",
    "            eos_token = '', \n",
    "            lower = True, \n",
    "            batch_first = True)\n",
    "\n",
    "TRG = Field(tokenize = getTokenizer, \n",
    "            init_token = '', \n",
    "            eos_token = '', \n",
    "            lower = False, \n",
    "            batch_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in tokenization\n",
      "Error in tokenization\n",
      "Error in tokenization\n",
      "Error in tokenization\n",
      "Error in tokenization\n"
     ]
    }
   ],
   "source": [
    "fields = [('src', SRC), ('trg', TRG)]\n",
    "\n",
    "Examples = [data.Example.fromlist([i,j], fields) for i,j in zip(questions,answers)]\n",
    "Dataset = data.Dataset(Examples, fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,valid_data = Dataset.split(split_ratio=[0.85,0.15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, min_freq = 1)\n",
    "TRG.build_vocab(train_data, min_freq = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1491\n",
      "1730\n"
     ]
    }
   ],
   "source": [
    "print(len(SRC.vocab))\n",
    "print(len(TRG.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumps dicts\n",
    "with open(\"./SRC_stio_local\",\"wb\") as f:\n",
    "  pickle.dump(SRC.vocab.stoi,f)\n",
    "with open(\"./TRG_itos_local\",\"wb\") as f:\n",
    "  pickle.dump(TRG.vocab.itos,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_iterator, valid_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data), \n",
    "     batch_size = BATCH_SIZE,\n",
    "     sort_key = lambda x: len(x.trg),\n",
    "     device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncodingComponent(nn.Module):\n",
    "    '''\n",
    "    Class to encode positional information to tokens.\n",
    "    \n",
    "\n",
    "    '''\n",
    "    def __init__(self,hid_dim,device,dropout=0.2,max_len=5000):\n",
    "        super().__init__()\n",
    "\n",
    "        assert hid_dim%2==0 # If not, it will result error in allocation to positional_encodings[:,1::2] later\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.positional_encodings = torch.zeros(max_len,hid_dim)\n",
    "\n",
    "        pos = torch.arange(0,max_len).unsqueeze(1) # pos : [max_len,1]\n",
    "        div_term  = torch.exp(-torch.arange(0,hid_dim,2)*math.log(10000.0)/hid_dim) # Calculating value of 1/(10000^(2i/hid_dim)) in log space and then exponentiating it\n",
    "        # div_term: [hid_dim//2]\n",
    "\n",
    "        self.positional_encodings[:,0::2] = torch.sin(pos*div_term) # pos*div_term [max_len,hid_dim//2]\n",
    "        self.positional_encodings[:,1::2] = torch.cos(pos*div_term) \n",
    "\n",
    "        self.positional_encodings = self.positional_encodings.unsqueeze(0) # To account for batch_size in inputs\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x + self.positional_encodings[:,:x.size(1)].detach().to(self.device)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardComponent(nn.Module):\n",
    "    '''\n",
    "    Class for pointwise feed forward connections\n",
    "    '''\n",
    "    def __init__(self,hid_dim,pf_dim,dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.fc1 = nn.Linear(hid_dim,pf_dim)\n",
    "        self.fc2 = nn.Linear(pf_dim,hid_dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        # x : [batch_size,seq_len,hid_dim]\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "\n",
    "        # x : [batch_size,seq_len,pf_dim]\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # x : [batch_size,seq_len,hid_dim]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttentionComponent(nn.Module):\n",
    "    '''\n",
    "    Multiheaded attention Component. This implementation also supports mask. \n",
    "    The reason for mask that in Decoder, we don't want attention mechanism to get\n",
    "    important information from future tokens.\n",
    "    '''\n",
    "    def __init__(self,hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "\n",
    "        assert hid_dim % n_heads == 0 # Since we split hid_dims into n_heads\n",
    "\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads # no of heads in 'multiheaded' attention\n",
    "        self.head_dim = hid_dim//n_heads # dims of each head\n",
    "\n",
    "        # Transformation from source vector to query vector\n",
    "        self.fc_q = nn.Linear(hid_dim,hid_dim)\n",
    "\n",
    "        # Transformation from source vector to key vector\n",
    "        self.fc_k = nn.Linear(hid_dim,hid_dim)\n",
    "\n",
    "        # Transformation from source vector to value vector\n",
    "        self.fc_v = nn.Linear(hid_dim,hid_dim)\n",
    "\n",
    "        self.fc_o = nn.Linear(hid_dim,hid_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Used in self attention for smoother gradients\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "\n",
    "    def forward(self,query,key,value,mask=None):\n",
    "\n",
    "        #query : [batch_size, query_len, hid_dim]\n",
    "        #key : [batch_size, key_len, hid_dim]\n",
    "        #value : [batch_size, value_len, hid_dim]\n",
    "\n",
    "        batch_size = query.shape[0]\n",
    "\n",
    "        # Transforming quey,key,values\n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "\n",
    "        #Q : [batch_size, query_len, hid_dim]\n",
    "        #K : [batch_size, key_len, hid_dim]\n",
    "        #V : [batch_size, value_len,hid_dim]\n",
    "\n",
    "        # Changing shapes to acocmadate n_heads information\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        #Q : [batch_size, n_heads, query_len, head_dim]\n",
    "        #K : [batch_size, n_heads, key_len, head_dim]\n",
    "        #V : [batch_size, n_heads, value_len, head_dim]\n",
    "\n",
    "        # Calculating alpha\n",
    "        score = torch.matmul(Q,K.permute(0,1,3,2))/self.scale\n",
    "        # score : [batch_size, n_heads, query_len, key_len]\n",
    "\n",
    "        if mask is not None:\n",
    "            score = score.masked_fill(mask==0,-1e10)\n",
    "\n",
    "        alpha = torch.softmax(score,dim=-1)\n",
    "        # alpha : [batch_size, n_heads, query_len, key_len]\n",
    "\n",
    "        # Get the final self-attention  vector\n",
    "        x = torch.matmul(self.dropout(alpha),V)\n",
    "        # x : [batch_size, n_heads, query_len, head_dim]\n",
    "\n",
    "        # Reshaping self attention vector to concatenate\n",
    "        x = x.permute(0,2,1,3).contiguous()\n",
    "        # x : [batch_size, query_len, n_heads, head_dim]\n",
    "\n",
    "        x = x.view(batch_size,-1,self.hid_dim)\n",
    "        # x: [batch_size, query_len, hid_dim]\n",
    "\n",
    "        # Transforming concatenated outputs \n",
    "        x = self.fc_o(x)\n",
    "        #x : [batch_size, query_len, hid_dim] \n",
    "\n",
    "        return x, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):  \n",
    "    '''\n",
    "    Operations of a single layer in an Encoder. An Encoder employs multiple such layers. Each layer contains:\n",
    "    1) multihead attention, folllowed by\n",
    "    2) LayerNorm of addition of multihead attention output and input to the layer, followed by\n",
    "    3) FeedForward connections, followed by\n",
    "    4) LayerNorm of addition of FeedForward outputs and output of previous layerNorm.\n",
    "    '''\n",
    "    def __init__(self, hid_dim,n_heads,pf_dim,dropout,device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn_layer_norm = nn. LayerNorm(hid_dim) #Layer norm after self-attention\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim) # Layer norm after FeedForward component\n",
    "\n",
    "        self.self_attention = MultiHeadedAttentionComponent(hid_dim,n_heads,dropout,device)\n",
    "        self.feed_forward = FeedForwardComponent(hid_dim,pf_dim,dropout)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,src,src_mask):\n",
    "        \n",
    "        # src : [batch_size, src_len, hid_dim]\n",
    "        # src_mask : [batch_size, 1, 1, src_len]\n",
    "\n",
    "        # get self-attention\n",
    "        _src, _ = self.self_attention(src,src,src,src_mask)\n",
    "\n",
    "        # LayerNorm after dropout\n",
    "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "        # src : [batch_size, src_len, hid_dim]\n",
    "\n",
    "        # FeedForward\n",
    "        _src = self.feed_forward(src)\n",
    "\n",
    "        # layerNorm after dropout\n",
    "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
    "        # src: [batch_size, src_len, hid_dim]\n",
    "\n",
    "        return src\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    '''\n",
    "    Operations of a single layer in an Decoder. An Decoder employs multiple such layers. Each layer contains:\n",
    "    1) masked decoder self attention, followed by\n",
    "    2) LayerNorm of addition of previous attention output and input to the layer,, followed by\n",
    "    3) encoder self attention, followed by\n",
    "    4) LayerNorm of addition of result of encoder self attention and its input, followed by\n",
    "    5) FeedForward connections, followed by\n",
    "    6) LayerNorm of addition of Feedforward results and its input.\n",
    "    '''\n",
    "    def __init__(self,hid_dim,n_heads,pf_dim,dropout,device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "\n",
    "        # decoder self attention\n",
    "        self.self_attention = MultiHeadedAttentionComponent(hid_dim,n_heads,dropout,device)\n",
    "\n",
    "        # encoder attention\n",
    "        self.encoder_attention = MultiHeadedAttentionComponent(hid_dim,n_heads,dropout,device)\n",
    "\n",
    "        # FeedForward\n",
    "        self.feed_forward = FeedForwardComponent(hid_dim,pf_dim,dropout)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,trg, enc_src,trg_mask,src_mask):\n",
    "\n",
    "        #trg : [batch_size, trg_len, hid_dim]\n",
    "        #enc_src : [batch_size, src_len, hid_dim]\n",
    "        #trg_mask : [batch_size, 1, trg_len, trg_len]\n",
    "        #src_mask : [batch_size, 1, 1, src_len]\n",
    "\n",
    "        '''\n",
    "        Decoder self-attention\n",
    "        trg_mask is to force decoder to look only into past tokens and not get information from future tokens.\n",
    "        Since we apply mask before doing softmax, the final self attention vector gets no information from future tokens.\n",
    "        '''\n",
    "        _trg, _ = self.self_attention(trg,trg,trg,trg_mask)\n",
    "\n",
    "        # LayerNorm and dropout with resdiual connection\n",
    "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        # trg : [batch_size, trg_len, hid_dim]\n",
    "\n",
    "        '''\n",
    "        Encoder attention:\n",
    "        Query: trg\n",
    "        key: enc_src\n",
    "        Value : enc_src\n",
    "        Why? \n",
    "        the idea here is to extract information from encoder outputs. So we use decoder self-attention as a query to find important values from enc_src\n",
    "        and that is why we use src_mask, to avoid getting information from enc_src positions where it is equal to pad-id\n",
    "        After we get necessary infromation from encoder outputs we add them back to decoder self-attention.\n",
    "        '''\n",
    "        _trg, encoder_attn_alpha = self.encoder_attention(trg,enc_src,enc_src,src_mask)\n",
    "\n",
    "            # LayerNorm , residual connection and dropout\n",
    "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        # trg : [ batch_size, trg_len, hid_dim]\n",
    "\n",
    "        # Feed Forward\n",
    "        _trg = self.feed_forward(trg)\n",
    "\n",
    "        # LayerNorm, residual connection and dropout\n",
    "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        return trg, encoder_attn_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    '''\n",
    "    An encoder, creates token embeddings and position embeddings and passes them through multiple encoder layers\n",
    "    '''\n",
    "    def __init__(self,input_dim,hid_dim,n_layers,n_heads,pf_dim,dropout,device,max_length = 5000):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.tok_embedding = nn.Embedding(input_dim,hid_dim)\n",
    "        self.pos_embedding = PositionalEncodingComponent(hid_dim,device,dropout,max_length)\n",
    "\n",
    "        # encoder layers\n",
    "        self.layers = nn.ModuleList([EncoderLayer(hid_dim,n_heads,pf_dim,dropout,device) for _ in range(n_layers)])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "\n",
    "    def forward(self,src,src_mask):\n",
    "\n",
    "        # src : [batch_size, src_len]\n",
    "        # src_mask : [batch_size,1,1,src_len]\n",
    "\n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "\n",
    "        tok_embeddings = self.tok_embedding(src)*self.scale\n",
    "\n",
    "        # token plus position embeddings\n",
    "        src  = self.pos_embedding(tok_embeddings)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            src = layer(src,src_mask)\n",
    "        # src : [batch_size, src_len, hid_dim]\n",
    "\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    '''\n",
    "    An decoder, creates token embeddings and position embeddings and passes them through multiple decoder layers\n",
    "    '''\n",
    "    def __init__(self,output_dim,hid_dim,n_layers,n_heads,pf_dim,dropout,device,max_length= 5000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.tok_embedding = nn.Embedding(output_dim,hid_dim)\n",
    "        self.pos_embedding = PositionalEncodingComponent(hid_dim,device,dropout,max_length)\n",
    "\n",
    "        # decoder layers\n",
    "        self.layers = nn.ModuleList([DecoderLayer(hid_dim,n_heads,pf_dim,dropout,device) for _ in range(n_layers)])\n",
    "\n",
    "        # convert decoder outputs to real outputs\n",
    "        self.fc_out = nn.Linear(hid_dim,output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "\n",
    "    def forward(self, trg, enc_src,trg_mask,src_mask):\n",
    "        \n",
    "        #trg : [batch_size, trg_len]\n",
    "        #enc_src : [batch_size, src_len, hid_dim]\n",
    "        #trg_mask : [batch_size, 1, trg_len, trg_len]\n",
    "        #src_mask : [batch_size, 1, 1, src_len]\n",
    "\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "\n",
    "        tok_embeddings = self.tok_embedding(trg)*self.scale\n",
    "\n",
    "        # token plus pos embeddings\n",
    "        trg = self.pos_embedding(tok_embeddings)\n",
    "        # trg : [batch_size, trg_len, hid_dim]\n",
    "\n",
    "        # Pass trg thorugh decoder layers\n",
    "        for layer in self.layers:\n",
    "            trg, encoder_attention = layer(trg,enc_src,trg_mask,src_mask)\n",
    "        \n",
    "        # trg : [batch_size,trg_len,hid_dim]\n",
    "        # encoder_attention :  [batch_size, n_head,trg_len, src_len]\n",
    "\n",
    "        # Convert to outputs\n",
    "        output = self.fc_out(trg)\n",
    "        # output : [batch_size, trg_len, output_dim]\n",
    "        \n",
    "        return output, encoder_attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self,src):\n",
    "        # src : [batch_size, src_len]\n",
    "\n",
    "        # Masking pad values\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # src_mask : [batch_size,1,1,src_len]\n",
    "\n",
    "        return src_mask\n",
    "\n",
    "    def make_trg_mask(self,trg):\n",
    "        # trg : [batch_size, trg_len]\n",
    "\n",
    "        # Masking pad values\n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # trg_pad_mask : [batch_size,1,1, trg_len]\n",
    "\n",
    "        # Masking future values\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len,trg_len),device= self.device)).bool()\n",
    "        # trg_sub_mask : [trg_len, trg_len]\n",
    "\n",
    "        # combine both masks\n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        # trg_mask = [batch_size,1,trg_len,trg_len]\n",
    "\n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self,src,trg):\n",
    "\n",
    "        # src : [batch_size, src_len]\n",
    "        # trg : [batch_size, trg_len]\n",
    "\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "\n",
    "        # src_mask : [ batch_size, 1,1,src_len]\n",
    "        # trg_mask : [batch_size, 1, trg_len, trg_len]\n",
    "\n",
    "        enc_src = self.encoder(src,src_mask)\n",
    "        #enc_src : [batch_size, src_len, hid_dim]\n",
    "\n",
    "        output, encoder_decoder_attention = self.decoder(trg,enc_src,trg_mask,src_mask)\n",
    "        # output : [batch_size, trg_len, output_dim]\n",
    "        # encoder_decoder_attention : [batch_size, n_heads, trg_len, src_len]\n",
    "\n",
    "        return output, encoder_decoder_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "HID_DIM = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "\n",
    "enc = Encoder(INPUT_DIM, \n",
    "              HID_DIM, \n",
    "              ENC_LAYERS, \n",
    "              ENC_HEADS, \n",
    "              ENC_PF_DIM, \n",
    "              ENC_DROPOUT, \n",
    "              device)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, \n",
    "              HID_DIM, \n",
    "              DEC_LAYERS, \n",
    "              DEC_HEADS, \n",
    "              DEC_PF_DIM, \n",
    "              DEC_DROPOUT, \n",
    "              device)\n",
    "\n",
    "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (tok_embedding): Embedding(1491, 256)\n",
       "    (pos_embedding): PositionalEncodingComponent(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadedAttentionComponent(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForwardComponent(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadedAttentionComponent(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForwardComponent(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadedAttentionComponent(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForwardComponent(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (tok_embedding): Embedding(1730, 256)\n",
       "    (pos_embedding): PositionalEncodingComponent(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadedAttentionComponent(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadedAttentionComponent(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForwardComponent(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadedAttentionComponent(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadedAttentionComponent(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForwardComponent(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadedAttentionComponent(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadedAttentionComponent(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForwardComponent(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc_out): Linear(in_features=256, out_features=1730, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(1730, 256)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decoder.tok_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(15018, 256)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load python embedding weights\n",
    "pretrained_embeddings = torch.load('./Data/python_embedding_weigts.pt')\n",
    "# pretrained_embeddings = torch.load('./best_train_loss.pt')\n",
    "pretrained_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading pretrained embeddings\n",
    "\n",
    "with open(\"./Data/TRG_stio\",\"rb\") as f:\n",
    "    stoi_weights = pickle.load(f)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        indexes = []\n",
    "        for i,j in enumerate(TRG.vocab.stoi):\n",
    "            if j in stoi_weights:\n",
    "                model.decoder.tok_embedding.weight[TRG.vocab.stoi[j]] = pretrained_embeddings.weight[stoi_weights[j]]\n",
    "            else:\n",
    "                model.decoder.tok_embedding.weight[TRG.vocab.stoi[j]] = pretrained_embeddings.weight[stoi_weights['']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 5,222,850 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rouge_score(output,trg,vocab=TRG.vocab.itos):\n",
    "    rouge = Rouge()\n",
    "    rouge_score = 0\n",
    "    argmax_outputs = output.argmax(2)\n",
    "    assert argmax_outputs.shape == trg.shape\n",
    "    with torch.no_grad():\n",
    "        for i,row in enumerate(argmax_outputs):\n",
    "            output_sentence = []\n",
    "            trg_sentence = []\n",
    "            for j,column in enumerate(row):\n",
    "                output_sentence.append(vocab[argmax_outputs[i][j]])\n",
    "                trg_sentence.append(vocab[trg[i][j]])\n",
    "            output_sentence = \"\".join(output_sentence)\n",
    "            trg_sentence = \"\".join(trg_sentence)\n",
    "\n",
    "            rouge_score += rouge.get_scores(output_sentence, trg_sentence)[0][\"rouge-l\"][\"f\"]\n",
    "\n",
    "    return rouge_score/len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, _ = model(src, trg[:,:-1])\n",
    "                \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #trg = [batch size, trg len]\n",
    "            \n",
    "        output_dim = output.shape[-1]\n",
    "            \n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:,1:].contiguous().view(-1)\n",
    "                \n",
    "        #output = [batch size * trg len - 1, output dim]\n",
    "        #trg = [batch size * trg len - 1]\n",
    "            \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "\n",
    "    epoch_rouge = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "            \n",
    "            #output = [batch size, trg len - 1, output dim]\n",
    "            #trg = [batch size, trg len]\n",
    "\n",
    "            rouge_score = get_rouge_score(output, trg[:,1:])\n",
    "\n",
    "            epoch_rouge += rouge_score\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            #output = [batch size * trg len - 1, output dim]\n",
    "            #trg = [batch size * trg len - 1]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        \n",
    "    return epoch_loss / len(iterator) , epoch_rouge / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 7s\n",
      "\tTrain Loss: 5.462 | Train PPL: 235.506\n",
      "\t Val. Loss: 4.870 |  Val. PPL: 130.303 | Val. Rouge:   0.000\n",
      "Epoch: 02 | Time: 0m 6s\n",
      "\tTrain Loss: 4.693 | Train PPL: 109.136\n",
      "\t Val. Loss: 4.150 |  Val. PPL:  63.433 | Val. Rouge:   0.001\n",
      "Epoch: 03 | Time: 0m 6s\n",
      "\tTrain Loss: 4.123 | Train PPL:  61.756\n",
      "\t Val. Loss: 3.729 |  Val. PPL:  41.643 | Val. Rouge:   0.001\n",
      "Epoch: 04 | Time: 0m 6s\n",
      "\tTrain Loss: 3.764 | Train PPL:  43.115\n",
      "\t Val. Loss: 3.492 |  Val. PPL:  32.847 | Val. Rouge:   0.002\n",
      "Epoch: 05 | Time: 0m 6s\n",
      "\tTrain Loss: 3.568 | Train PPL:  35.446\n",
      "\t Val. Loss: 3.317 |  Val. PPL:  27.568 | Val. Rouge:   0.003\n",
      "Epoch: 06 | Time: 0m 6s\n",
      "\tTrain Loss: 3.394 | Train PPL:  29.788\n",
      "\t Val. Loss: 3.176 |  Val. PPL:  23.941 | Val. Rouge:   0.003\n",
      "Epoch: 07 | Time: 0m 6s\n",
      "\tTrain Loss: 3.271 | Train PPL:  26.329\n",
      "\t Val. Loss: 3.060 |  Val. PPL:  21.333 | Val. Rouge:   0.005\n",
      "Epoch: 08 | Time: 0m 6s\n",
      "\tTrain Loss: 3.142 | Train PPL:  23.141\n",
      "\t Val. Loss: 2.986 |  Val. PPL:  19.800 | Val. Rouge:   0.005\n",
      "Epoch: 09 | Time: 0m 6s\n",
      "\tTrain Loss: 3.031 | Train PPL:  20.709\n",
      "\t Val. Loss: 2.910 |  Val. PPL:  18.351 | Val. Rouge:   0.009\n",
      "Epoch: 10 | Time: 0m 6s\n",
      "\tTrain Loss: 2.965 | Train PPL:  19.400\n",
      "\t Val. Loss: 2.856 |  Val. PPL:  17.392 | Val. Rouge:   0.009\n",
      "Epoch: 11 | Time: 0m 6s\n",
      "\tTrain Loss: 2.885 | Train PPL:  17.901\n",
      "\t Val. Loss: 2.810 |  Val. PPL:  16.616 | Val. Rouge:   0.009\n",
      "Epoch: 12 | Time: 0m 6s\n",
      "\tTrain Loss: 2.829 | Train PPL:  16.936\n",
      "\t Val. Loss: 2.765 |  Val. PPL:  15.882 | Val. Rouge:   0.007\n",
      "Epoch: 13 | Time: 0m 6s\n",
      "\tTrain Loss: 2.762 | Train PPL:  15.839\n",
      "\t Val. Loss: 2.720 |  Val. PPL:  15.185 | Val. Rouge:   0.010\n",
      "Epoch: 14 | Time: 0m 6s\n",
      "\tTrain Loss: 2.704 | Train PPL:  14.944\n",
      "\t Val. Loss: 2.685 |  Val. PPL:  14.664 | Val. Rouge:   0.009\n",
      "Epoch: 15 | Time: 0m 6s\n",
      "\tTrain Loss: 2.641 | Train PPL:  14.031\n",
      "\t Val. Loss: 2.656 |  Val. PPL:  14.242 | Val. Rouge:   0.012\n",
      "Epoch: 16 | Time: 0m 6s\n",
      "\tTrain Loss: 2.606 | Train PPL:  13.540\n",
      "\t Val. Loss: 2.634 |  Val. PPL:  13.926 | Val. Rouge:   0.014\n",
      "Epoch: 17 | Time: 0m 6s\n",
      "\tTrain Loss: 2.567 | Train PPL:  13.022\n",
      "\t Val. Loss: 2.616 |  Val. PPL:  13.678 | Val. Rouge:   0.015\n",
      "Epoch: 18 | Time: 0m 6s\n",
      "\tTrain Loss: 2.513 | Train PPL:  12.338\n",
      "\t Val. Loss: 2.581 |  Val. PPL:  13.213 | Val. Rouge:   0.014\n",
      "Epoch: 19 | Time: 0m 6s\n",
      "\tTrain Loss: 2.459 | Train PPL:  11.696\n",
      "\t Val. Loss: 2.568 |  Val. PPL:  13.045 | Val. Rouge:   0.014\n",
      "Epoch: 20 | Time: 0m 6s\n",
      "\tTrain Loss: 2.428 | Train PPL:  11.335\n",
      "\t Val. Loss: 2.545 |  Val. PPL:  12.738 | Val. Rouge:   0.013\n",
      "Epoch: 21 | Time: 0m 6s\n",
      "\tTrain Loss: 2.382 | Train PPL:  10.831\n",
      "\t Val. Loss: 2.522 |  Val. PPL:  12.450 | Val. Rouge:   0.013\n",
      "Epoch: 22 | Time: 0m 6s\n",
      "\tTrain Loss: 2.360 | Train PPL:  10.590\n",
      "\t Val. Loss: 2.521 |  Val. PPL:  12.439 | Val. Rouge:   0.018\n",
      "Epoch: 23 | Time: 0m 6s\n",
      "\tTrain Loss: 2.303 | Train PPL:  10.006\n",
      "\t Val. Loss: 2.499 |  Val. PPL:  12.164 | Val. Rouge:   0.015\n",
      "Epoch: 24 | Time: 0m 6s\n",
      "\tTrain Loss: 2.299 | Train PPL:   9.961\n",
      "\t Val. Loss: 2.490 |  Val. PPL:  12.063 | Val. Rouge:   0.018\n",
      "Epoch: 25 | Time: 0m 6s\n",
      "\tTrain Loss: 2.253 | Train PPL:   9.515\n",
      "\t Val. Loss: 2.481 |  Val. PPL:  11.955 | Val. Rouge:   0.018\n",
      "Epoch: 26 | Time: 0m 6s\n",
      "\tTrain Loss: 2.219 | Train PPL:   9.195\n",
      "\t Val. Loss: 2.459 |  Val. PPL:  11.692 | Val. Rouge:   0.017\n",
      "Epoch: 27 | Time: 0m 6s\n",
      "\tTrain Loss: 2.215 | Train PPL:   9.160\n",
      "\t Val. Loss: 2.446 |  Val. PPL:  11.544 | Val. Rouge:   0.020\n",
      "Epoch: 28 | Time: 0m 6s\n",
      "\tTrain Loss: 2.153 | Train PPL:   8.608\n",
      "\t Val. Loss: 2.441 |  Val. PPL:  11.481 | Val. Rouge:   0.019\n",
      "Epoch: 29 | Time: 0m 6s\n",
      "\tTrain Loss: 2.120 | Train PPL:   8.334\n",
      "\t Val. Loss: 2.432 |  Val. PPL:  11.381 | Val. Rouge:   0.023\n",
      "Epoch: 30 | Time: 0m 6s\n",
      "\tTrain Loss: 2.101 | Train PPL:   8.177\n",
      "\t Val. Loss: 2.433 |  Val. PPL:  11.397 | Val. Rouge:   0.022\n",
      "Epoch: 31 | Time: 0m 6s\n",
      "\tTrain Loss: 2.056 | Train PPL:   7.812\n",
      "\t Val. Loss: 2.421 |  Val. PPL:  11.260 | Val. Rouge:   0.020\n",
      "Epoch: 32 | Time: 0m 6s\n",
      "\tTrain Loss: 2.029 | Train PPL:   7.607\n",
      "\t Val. Loss: 2.417 |  Val. PPL:  11.209 | Val. Rouge:   0.026\n",
      "Epoch: 33 | Time: 0m 6s\n",
      "\tTrain Loss: 2.024 | Train PPL:   7.571\n",
      "\t Val. Loss: 2.410 |  Val. PPL:  11.132 | Val. Rouge:   0.026\n",
      "Epoch: 34 | Time: 0m 6s\n",
      "\tTrain Loss: 1.992 | Train PPL:   7.332\n",
      "\t Val. Loss: 2.403 |  Val. PPL:  11.058 | Val. Rouge:   0.027\n",
      "Epoch: 35 | Time: 0m 6s\n",
      "\tTrain Loss: 1.977 | Train PPL:   7.218\n",
      "\t Val. Loss: 2.405 |  Val. PPL:  11.083 | Val. Rouge:   0.026\n",
      "Epoch: 36 | Time: 0m 6s\n",
      "\tTrain Loss: 1.945 | Train PPL:   6.990\n",
      "\t Val. Loss: 2.402 |  Val. PPL:  11.048 | Val. Rouge:   0.031\n",
      "Epoch: 37 | Time: 0m 6s\n",
      "\tTrain Loss: 1.929 | Train PPL:   6.882\n",
      "\t Val. Loss: 2.392 |  Val. PPL:  10.936 | Val. Rouge:   0.028\n",
      "Epoch: 38 | Time: 0m 6s\n",
      "\tTrain Loss: 1.909 | Train PPL:   6.743\n",
      "\t Val. Loss: 2.388 |  Val. PPL:  10.893 | Val. Rouge:   0.027\n",
      "Epoch: 39 | Time: 0m 6s\n",
      "\tTrain Loss: 1.866 | Train PPL:   6.465\n",
      "\t Val. Loss: 2.384 |  Val. PPL:  10.850 | Val. Rouge:   0.027\n",
      "Epoch: 40 | Time: 0m 6s\n",
      "\tTrain Loss: 1.833 | Train PPL:   6.253\n",
      "\t Val. Loss: 2.391 |  Val. PPL:  10.923 | Val. Rouge:   0.027\n",
      "Epoch: 41 | Time: 0m 6s\n",
      "\tTrain Loss: 1.836 | Train PPL:   6.274\n",
      "\t Val. Loss: 2.380 |  Val. PPL:  10.809 | Val. Rouge:   0.029\n",
      "Epoch: 42 | Time: 0m 6s\n",
      "\tTrain Loss: 1.804 | Train PPL:   6.074\n",
      "\t Val. Loss: 2.374 |  Val. PPL:  10.743 | Val. Rouge:   0.030\n",
      "Epoch: 43 | Time: 0m 6s\n",
      "\tTrain Loss: 1.775 | Train PPL:   5.902\n",
      "\t Val. Loss: 2.366 |  Val. PPL:  10.656 | Val. Rouge:   0.028\n",
      "Epoch: 44 | Time: 0m 6s\n",
      "\tTrain Loss: 1.763 | Train PPL:   5.832\n",
      "\t Val. Loss: 2.380 |  Val. PPL:  10.805 | Val. Rouge:   0.029\n",
      "Epoch: 45 | Time: 0m 6s\n",
      "\tTrain Loss: 1.755 | Train PPL:   5.782\n",
      "\t Val. Loss: 2.372 |  Val. PPL:  10.722 | Val. Rouge:   0.032\n",
      "Epoch: 46 | Time: 0m 6s\n",
      "\tTrain Loss: 1.711 | Train PPL:   5.536\n",
      "\t Val. Loss: 2.381 |  Val. PPL:  10.817 | Val. Rouge:   0.030\n",
      "Epoch: 47 | Time: 0m 6s\n",
      "\tTrain Loss: 1.699 | Train PPL:   5.466\n",
      "\t Val. Loss: 2.371 |  Val. PPL:  10.705 | Val. Rouge:   0.031\n",
      "Epoch: 48 | Time: 0m 6s\n",
      "\tTrain Loss: 1.691 | Train PPL:   5.425\n",
      "\t Val. Loss: 2.370 |  Val. PPL:  10.700 | Val. Rouge:   0.028\n",
      "Epoch: 49 | Time: 0m 6s\n",
      "\tTrain Loss: 1.660 | Train PPL:   5.261\n",
      "\t Val. Loss: 2.366 |  Val. PPL:  10.659 | Val. Rouge:   0.031\n",
      "Epoch: 50 | Time: 0m 6s\n",
      "\tTrain Loss: 1.650 | Train PPL:   5.206\n",
      "\t Val. Loss: 2.383 |  Val. PPL:  10.840 | Val. Rouge:   0.031\n",
      "Epoch: 51 | Time: 0m 6s\n",
      "\tTrain Loss: 1.632 | Train PPL:   5.115\n",
      "\t Val. Loss: 2.368 |  Val. PPL:  10.681 | Val. Rouge:   0.030\n",
      "Epoch: 52 | Time: 0m 6s\n",
      "\tTrain Loss: 1.603 | Train PPL:   4.967\n",
      "\t Val. Loss: 2.366 |  Val. PPL:  10.658 | Val. Rouge:   0.031\n",
      "Epoch: 53 | Time: 0m 6s\n",
      "\tTrain Loss: 1.579 | Train PPL:   4.851\n",
      "\t Val. Loss: 2.370 |  Val. PPL:  10.701 | Val. Rouge:   0.033\n",
      "Epoch: 54 | Time: 0m 6s\n",
      "\tTrain Loss: 1.583 | Train PPL:   4.871\n",
      "\t Val. Loss: 2.362 |  Val. PPL:  10.617 | Val. Rouge:   0.033\n",
      "Epoch: 55 | Time: 0m 6s\n",
      "\tTrain Loss: 1.547 | Train PPL:   4.699\n",
      "\t Val. Loss: 2.383 |  Val. PPL:  10.837 | Val. Rouge:   0.033\n",
      "Epoch: 56 | Time: 0m 6s\n",
      "\tTrain Loss: 1.549 | Train PPL:   4.705\n",
      "\t Val. Loss: 2.372 |  Val. PPL:  10.720 | Val. Rouge:   0.035\n",
      "Epoch: 57 | Time: 0m 6s\n",
      "\tTrain Loss: 1.531 | Train PPL:   4.621\n",
      "\t Val. Loss: 2.373 |  Val. PPL:  10.726 | Val. Rouge:   0.034\n",
      "Epoch: 58 | Time: 0m 6s\n",
      "\tTrain Loss: 1.501 | Train PPL:   4.486\n",
      "\t Val. Loss: 2.385 |  Val. PPL:  10.857 | Val. Rouge:   0.035\n",
      "Epoch: 59 | Time: 0m 6s\n",
      "\tTrain Loss: 1.479 | Train PPL:   4.390\n",
      "\t Val. Loss: 2.382 |  Val. PPL:  10.832 | Val. Rouge:   0.038\n",
      "Epoch: 60 | Time: 0m 6s\n",
      "\tTrain Loss: 1.463 | Train PPL:   4.317\n",
      "\t Val. Loss: 2.386 |  Val. PPL:  10.873 | Val. Rouge:   0.038\n",
      "Epoch: 61 | Time: 0m 6s\n",
      "\tTrain Loss: 1.459 | Train PPL:   4.302\n",
      "\t Val. Loss: 2.385 |  Val. PPL:  10.863 | Val. Rouge:   0.038\n",
      "Epoch: 62 | Time: 0m 6s\n",
      "\tTrain Loss: 1.426 | Train PPL:   4.160\n",
      "\t Val. Loss: 2.396 |  Val. PPL:  10.977 | Val. Rouge:   0.034\n",
      "Epoch: 63 | Time: 0m 6s\n",
      "\tTrain Loss: 1.428 | Train PPL:   4.172\n",
      "\t Val. Loss: 2.391 |  Val. PPL:  10.920 | Val. Rouge:   0.038\n",
      "Epoch: 64 | Time: 0m 6s\n",
      "\tTrain Loss: 1.399 | Train PPL:   4.051\n",
      "\t Val. Loss: 2.398 |  Val. PPL:  11.005 | Val. Rouge:   0.035\n",
      "Epoch: 65 | Time: 0m 6s\n",
      "\tTrain Loss: 1.406 | Train PPL:   4.079\n",
      "\t Val. Loss: 2.402 |  Val. PPL:  11.041 | Val. Rouge:   0.039\n",
      "Epoch: 66 | Time: 0m 6s\n",
      "\tTrain Loss: 1.378 | Train PPL:   3.967\n",
      "\t Val. Loss: 2.390 |  Val. PPL:  10.912 | Val. Rouge:   0.039\n",
      "Epoch: 67 | Time: 0m 6s\n",
      "\tTrain Loss: 1.362 | Train PPL:   3.906\n",
      "\t Val. Loss: 2.409 |  Val. PPL:  11.117 | Val. Rouge:   0.038\n",
      "Epoch: 68 | Time: 0m 6s\n",
      "\tTrain Loss: 1.349 | Train PPL:   3.854\n",
      "\t Val. Loss: 2.385 |  Val. PPL:  10.863 | Val. Rouge:   0.037\n",
      "Epoch: 69 | Time: 0m 6s\n",
      "\tTrain Loss: 1.349 | Train PPL:   3.853\n",
      "\t Val. Loss: 2.386 |  Val. PPL:  10.866 | Val. Rouge:   0.038\n",
      "Epoch: 70 | Time: 0m 6s\n",
      "\tTrain Loss: 1.304 | Train PPL:   3.684\n",
      "\t Val. Loss: 2.396 |  Val. PPL:  10.978 | Val. Rouge:   0.045\n",
      "Epoch: 71 | Time: 0m 6s\n",
      "\tTrain Loss: 1.297 | Train PPL:   3.659\n",
      "\t Val. Loss: 2.408 |  Val. PPL:  11.107 | Val. Rouge:   0.041\n",
      "Epoch: 72 | Time: 0m 6s\n",
      "\tTrain Loss: 1.309 | Train PPL:   3.702\n",
      "\t Val. Loss: 2.402 |  Val. PPL:  11.044 | Val. Rouge:   0.041\n",
      "Epoch: 73 | Time: 0m 6s\n",
      "\tTrain Loss: 1.266 | Train PPL:   3.545\n",
      "\t Val. Loss: 2.416 |  Val. PPL:  11.200 | Val. Rouge:   0.039\n",
      "Epoch: 74 | Time: 0m 6s\n",
      "\tTrain Loss: 1.274 | Train PPL:   3.575\n",
      "\t Val. Loss: 2.427 |  Val. PPL:  11.328 | Val. Rouge:   0.038\n",
      "Epoch: 75 | Time: 0m 6s\n",
      "\tTrain Loss: 1.250 | Train PPL:   3.489\n",
      "\t Val. Loss: 2.422 |  Val. PPL:  11.270 | Val. Rouge:   0.044\n",
      "Epoch: 76 | Time: 0m 6s\n",
      "\tTrain Loss: 1.238 | Train PPL:   3.450\n",
      "\t Val. Loss: 2.438 |  Val. PPL:  11.445 | Val. Rouge:   0.043\n",
      "Epoch: 77 | Time: 0m 6s\n",
      "\tTrain Loss: 1.207 | Train PPL:   3.342\n",
      "\t Val. Loss: 2.415 |  Val. PPL:  11.190 | Val. Rouge:   0.040\n",
      "Epoch: 78 | Time: 0m 6s\n",
      "\tTrain Loss: 1.214 | Train PPL:   3.368\n",
      "\t Val. Loss: 2.416 |  Val. PPL:  11.196 | Val. Rouge:   0.044\n",
      "Epoch: 79 | Time: 0m 6s\n",
      "\tTrain Loss: 1.200 | Train PPL:   3.320\n",
      "\t Val. Loss: 2.430 |  Val. PPL:  11.358 | Val. Rouge:   0.041\n",
      "Epoch: 80 | Time: 0m 6s\n",
      "\tTrain Loss: 1.195 | Train PPL:   3.302\n",
      "\t Val. Loss: 2.440 |  Val. PPL:  11.470 | Val. Rouge:   0.044\n",
      "Epoch: 81 | Time: 0m 6s\n",
      "\tTrain Loss: 1.168 | Train PPL:   3.215\n",
      "\t Val. Loss: 2.461 |  Val. PPL:  11.714 | Val. Rouge:   0.041\n",
      "Epoch: 82 | Time: 0m 6s\n",
      "\tTrain Loss: 1.153 | Train PPL:   3.168\n",
      "\t Val. Loss: 2.450 |  Val. PPL:  11.589 | Val. Rouge:   0.045\n",
      "Epoch: 83 | Time: 0m 6s\n",
      "\tTrain Loss: 1.143 | Train PPL:   3.137\n",
      "\t Val. Loss: 2.459 |  Val. PPL:  11.696 | Val. Rouge:   0.044\n",
      "Epoch: 84 | Time: 0m 6s\n",
      "\tTrain Loss: 1.158 | Train PPL:   3.183\n",
      "\t Val. Loss: 2.445 |  Val. PPL:  11.528 | Val. Rouge:   0.044\n",
      "Epoch: 85 | Time: 0m 6s\n",
      "\tTrain Loss: 1.123 | Train PPL:   3.075\n",
      "\t Val. Loss: 2.460 |  Val. PPL:  11.699 | Val. Rouge:   0.048\n",
      "Epoch: 86 | Time: 0m 6s\n",
      "\tTrain Loss: 1.113 | Train PPL:   3.044\n",
      "\t Val. Loss: 2.461 |  Val. PPL:  11.712 | Val. Rouge:   0.044\n",
      "Epoch: 87 | Time: 0m 7s\n",
      "\tTrain Loss: 1.118 | Train PPL:   3.058\n",
      "\t Val. Loss: 2.477 |  Val. PPL:  11.905 | Val. Rouge:   0.047\n",
      "Epoch: 88 | Time: 0m 6s\n",
      "\tTrain Loss: 1.096 | Train PPL:   2.993\n",
      "\t Val. Loss: 2.465 |  Val. PPL:  11.765 | Val. Rouge:   0.044\n",
      "Epoch: 89 | Time: 0m 7s\n",
      "\tTrain Loss: 1.100 | Train PPL:   3.006\n",
      "\t Val. Loss: 2.470 |  Val. PPL:  11.819 | Val. Rouge:   0.044\n",
      "Epoch: 90 | Time: 0m 6s\n",
      "\tTrain Loss: 1.077 | Train PPL:   2.936\n",
      "\t Val. Loss: 2.474 |  Val. PPL:  11.869 | Val. Rouge:   0.050\n",
      "Epoch: 91 | Time: 0m 6s\n",
      "\tTrain Loss: 1.054 | Train PPL:   2.870\n",
      "\t Val. Loss: 2.482 |  Val. PPL:  11.962 | Val. Rouge:   0.047\n",
      "Epoch: 92 | Time: 0m 6s\n",
      "\tTrain Loss: 1.048 | Train PPL:   2.853\n",
      "\t Val. Loss: 2.494 |  Val. PPL:  12.109 | Val. Rouge:   0.049\n",
      "Epoch: 93 | Time: 0m 6s\n",
      "\tTrain Loss: 1.057 | Train PPL:   2.878\n",
      "\t Val. Loss: 2.489 |  Val. PPL:  12.051 | Val. Rouge:   0.048\n",
      "Epoch: 94 | Time: 0m 6s\n",
      "\tTrain Loss: 1.028 | Train PPL:   2.795\n",
      "\t Val. Loss: 2.489 |  Val. PPL:  12.047 | Val. Rouge:   0.049\n",
      "Epoch: 95 | Time: 0m 6s\n",
      "\tTrain Loss: 1.038 | Train PPL:   2.824\n",
      "\t Val. Loss: 2.497 |  Val. PPL:  12.151 | Val. Rouge:   0.044\n",
      "Epoch: 96 | Time: 0m 6s\n",
      "\tTrain Loss: 1.025 | Train PPL:   2.788\n",
      "\t Val. Loss: 2.500 |  Val. PPL:  12.180 | Val. Rouge:   0.050\n",
      "Epoch: 97 | Time: 0m 6s\n",
      "\tTrain Loss: 0.998 | Train PPL:   2.712\n",
      "\t Val. Loss: 2.505 |  Val. PPL:  12.248 | Val. Rouge:   0.049\n",
      "Epoch: 98 | Time: 0m 6s\n",
      "\tTrain Loss: 1.000 | Train PPL:   2.717\n",
      "\t Val. Loss: 2.508 |  Val. PPL:  12.279 | Val. Rouge:   0.049\n",
      "Epoch: 99 | Time: 0m 7s\n",
      "\tTrain Loss: 0.979 | Train PPL:   2.663\n",
      "\t Val. Loss: 2.521 |  Val. PPL:  12.442 | Val. Rouge:   0.048\n",
      "Epoch: 100 | Time: 0m 6s\n",
      "\tTrain Loss: 0.977 | Train PPL:   2.657\n",
      "\t Val. Loss: 2.532 |  Val. PPL:  12.582 | Val. Rouge:   0.047\n",
      "Epoch: 101 | Time: 0m 7s\n",
      "\tTrain Loss: 0.967 | Train PPL:   2.631\n",
      "\t Val. Loss: 2.534 |  Val. PPL:  12.610 | Val. Rouge:   0.048\n",
      "Epoch: 102 | Time: 0m 7s\n",
      "\tTrain Loss: 0.960 | Train PPL:   2.611\n",
      "\t Val. Loss: 2.521 |  Val. PPL:  12.438 | Val. Rouge:   0.045\n",
      "Epoch: 103 | Time: 0m 7s\n",
      "\tTrain Loss: 0.941 | Train PPL:   2.562\n",
      "\t Val. Loss: 2.546 |  Val. PPL:  12.758 | Val. Rouge:   0.046\n",
      "Epoch: 104 | Time: 0m 7s\n",
      "\tTrain Loss: 0.935 | Train PPL:   2.546\n",
      "\t Val. Loss: 2.534 |  Val. PPL:  12.605 | Val. Rouge:   0.049\n",
      "Epoch: 105 | Time: 0m 6s\n",
      "\tTrain Loss: 0.921 | Train PPL:   2.511\n",
      "\t Val. Loss: 2.545 |  Val. PPL:  12.746 | Val. Rouge:   0.047\n",
      "Epoch: 106 | Time: 0m 7s\n",
      "\tTrain Loss: 0.921 | Train PPL:   2.511\n",
      "\t Val. Loss: 2.555 |  Val. PPL:  12.871 | Val. Rouge:   0.050\n",
      "Epoch: 107 | Time: 0m 6s\n",
      "\tTrain Loss: 0.911 | Train PPL:   2.486\n",
      "\t Val. Loss: 2.568 |  Val. PPL:  13.042 | Val. Rouge:   0.051\n",
      "Epoch: 108 | Time: 0m 6s\n",
      "\tTrain Loss: 0.887 | Train PPL:   2.428\n",
      "\t Val. Loss: 2.566 |  Val. PPL:  13.016 | Val. Rouge:   0.046\n",
      "Epoch: 109 | Time: 0m 6s\n",
      "\tTrain Loss: 0.889 | Train PPL:   2.433\n",
      "\t Val. Loss: 2.562 |  Val. PPL:  12.962 | Val. Rouge:   0.047\n",
      "Epoch: 110 | Time: 0m 6s\n",
      "\tTrain Loss: 0.884 | Train PPL:   2.419\n",
      "\t Val. Loss: 2.558 |  Val. PPL:  12.907 | Val. Rouge:   0.052\n",
      "Epoch: 111 | Time: 0m 6s\n",
      "\tTrain Loss: 0.876 | Train PPL:   2.402\n",
      "\t Val. Loss: 2.579 |  Val. PPL:  13.183 | Val. Rouge:   0.053\n",
      "Epoch: 112 | Time: 0m 7s\n",
      "\tTrain Loss: 0.857 | Train PPL:   2.356\n",
      "\t Val. Loss: 2.590 |  Val. PPL:  13.335 | Val. Rouge:   0.052\n",
      "Epoch: 113 | Time: 0m 7s\n",
      "\tTrain Loss: 0.852 | Train PPL:   2.344\n",
      "\t Val. Loss: 2.576 |  Val. PPL:  13.146 | Val. Rouge:   0.047\n",
      "Epoch: 114 | Time: 0m 6s\n",
      "\tTrain Loss: 0.846 | Train PPL:   2.331\n",
      "\t Val. Loss: 2.592 |  Val. PPL:  13.361 | Val. Rouge:   0.052\n",
      "Epoch: 115 | Time: 0m 6s\n",
      "\tTrain Loss: 0.837 | Train PPL:   2.309\n",
      "\t Val. Loss: 2.612 |  Val. PPL:  13.629 | Val. Rouge:   0.048\n",
      "Epoch: 116 | Time: 0m 6s\n",
      "\tTrain Loss: 0.821 | Train PPL:   2.272\n",
      "\t Val. Loss: 2.596 |  Val. PPL:  13.406 | Val. Rouge:   0.052\n",
      "Epoch: 117 | Time: 0m 6s\n",
      "\tTrain Loss: 0.840 | Train PPL:   2.316\n",
      "\t Val. Loss: 2.615 |  Val. PPL:  13.661 | Val. Rouge:   0.050\n",
      "Epoch: 118 | Time: 0m 6s\n",
      "\tTrain Loss: 0.807 | Train PPL:   2.241\n",
      "\t Val. Loss: 2.626 |  Val. PPL:  13.825 | Val. Rouge:   0.054\n",
      "Epoch: 119 | Time: 0m 7s\n",
      "\tTrain Loss: 0.810 | Train PPL:   2.248\n",
      "\t Val. Loss: 2.634 |  Val. PPL:  13.927 | Val. Rouge:   0.053\n",
      "Epoch: 120 | Time: 0m 7s\n",
      "\tTrain Loss: 0.810 | Train PPL:   2.248\n",
      "\t Val. Loss: 2.628 |  Val. PPL:  13.841 | Val. Rouge:   0.053\n",
      "Epoch: 121 | Time: 0m 7s\n",
      "\tTrain Loss: 0.792 | Train PPL:   2.209\n",
      "\t Val. Loss: 2.635 |  Val. PPL:  13.949 | Val. Rouge:   0.048\n",
      "Epoch: 122 | Time: 0m 6s\n",
      "\tTrain Loss: 0.794 | Train PPL:   2.212\n",
      "\t Val. Loss: 2.651 |  Val. PPL:  14.167 | Val. Rouge:   0.050\n",
      "Epoch: 123 | Time: 0m 6s\n",
      "\tTrain Loss: 0.771 | Train PPL:   2.162\n",
      "\t Val. Loss: 2.641 |  Val. PPL:  14.020 | Val. Rouge:   0.054\n",
      "Epoch: 124 | Time: 0m 6s\n",
      "\tTrain Loss: 0.764 | Train PPL:   2.148\n",
      "\t Val. Loss: 2.681 |  Val. PPL:  14.603 | Val. Rouge:   0.055\n",
      "Epoch: 125 | Time: 0m 6s\n",
      "\tTrain Loss: 0.760 | Train PPL:   2.139\n",
      "\t Val. Loss: 2.662 |  Val. PPL:  14.326 | Val. Rouge:   0.052\n",
      "Epoch: 126 | Time: 0m 6s\n",
      "\tTrain Loss: 0.763 | Train PPL:   2.145\n",
      "\t Val. Loss: 2.652 |  Val. PPL:  14.179 | Val. Rouge:   0.053\n",
      "Epoch: 127 | Time: 0m 6s\n",
      "\tTrain Loss: 0.752 | Train PPL:   2.121\n",
      "\t Val. Loss: 2.668 |  Val. PPL:  14.411 | Val. Rouge:   0.050\n",
      "Epoch: 128 | Time: 0m 7s\n",
      "\tTrain Loss: 0.748 | Train PPL:   2.113\n",
      "\t Val. Loss: 2.670 |  Val. PPL:  14.440 | Val. Rouge:   0.054\n",
      "Epoch: 129 | Time: 0m 6s\n",
      "\tTrain Loss: 0.733 | Train PPL:   2.082\n",
      "\t Val. Loss: 2.672 |  Val. PPL:  14.476 | Val. Rouge:   0.055\n",
      "Epoch: 130 | Time: 0m 6s\n",
      "\tTrain Loss: 0.718 | Train PPL:   2.051\n",
      "\t Val. Loss: 2.683 |  Val. PPL:  14.623 | Val. Rouge:   0.053\n",
      "Epoch: 131 | Time: 0m 6s\n",
      "\tTrain Loss: 0.728 | Train PPL:   2.071\n",
      "\t Val. Loss: 2.690 |  Val. PPL:  14.736 | Val. Rouge:   0.054\n",
      "Epoch: 132 | Time: 0m 6s\n",
      "\tTrain Loss: 0.708 | Train PPL:   2.030\n",
      "\t Val. Loss: 2.697 |  Val. PPL:  14.833 | Val. Rouge:   0.056\n",
      "Epoch: 133 | Time: 0m 7s\n",
      "\tTrain Loss: 0.716 | Train PPL:   2.046\n",
      "\t Val. Loss: 2.696 |  Val. PPL:  14.827 | Val. Rouge:   0.053\n",
      "Epoch: 134 | Time: 0m 7s\n",
      "\tTrain Loss: 0.713 | Train PPL:   2.039\n",
      "\t Val. Loss: 2.700 |  Val. PPL:  14.886 | Val. Rouge:   0.054\n",
      "Epoch: 135 | Time: 0m 6s\n",
      "\tTrain Loss: 0.709 | Train PPL:   2.032\n",
      "\t Val. Loss: 2.699 |  Val. PPL:  14.869 | Val. Rouge:   0.055\n",
      "Epoch: 136 | Time: 0m 6s\n",
      "\tTrain Loss: 0.691 | Train PPL:   1.995\n",
      "\t Val. Loss: 2.716 |  Val. PPL:  15.125 | Val. Rouge:   0.052\n",
      "Epoch: 137 | Time: 0m 6s\n",
      "\tTrain Loss: 0.679 | Train PPL:   1.973\n",
      "\t Val. Loss: 2.722 |  Val. PPL:  15.216 | Val. Rouge:   0.057\n",
      "Epoch: 138 | Time: 0m 7s\n",
      "\tTrain Loss: 0.682 | Train PPL:   1.978\n",
      "\t Val. Loss: 2.724 |  Val. PPL:  15.241 | Val. Rouge:   0.050\n",
      "Epoch: 139 | Time: 0m 6s\n",
      "\tTrain Loss: 0.677 | Train PPL:   1.967\n",
      "\t Val. Loss: 2.732 |  Val. PPL:  15.366 | Val. Rouge:   0.054\n",
      "Epoch: 140 | Time: 0m 6s\n",
      "\tTrain Loss: 0.658 | Train PPL:   1.931\n",
      "\t Val. Loss: 2.745 |  Val. PPL:  15.571 | Val. Rouge:   0.054\n",
      "Epoch: 141 | Time: 0m 6s\n",
      "\tTrain Loss: 0.660 | Train PPL:   1.934\n",
      "\t Val. Loss: 2.725 |  Val. PPL:  15.262 | Val. Rouge:   0.056\n",
      "Epoch: 142 | Time: 0m 6s\n",
      "\tTrain Loss: 0.654 | Train PPL:   1.924\n",
      "\t Val. Loss: 2.755 |  Val. PPL:  15.727 | Val. Rouge:   0.054\n",
      "Epoch: 143 | Time: 0m 6s\n",
      "\tTrain Loss: 0.646 | Train PPL:   1.908\n",
      "\t Val. Loss: 2.743 |  Val. PPL:  15.537 | Val. Rouge:   0.055\n",
      "Epoch: 144 | Time: 0m 6s\n",
      "\tTrain Loss: 0.649 | Train PPL:   1.913\n",
      "\t Val. Loss: 2.758 |  Val. PPL:  15.762 | Val. Rouge:   0.055\n",
      "Epoch: 145 | Time: 0m 6s\n",
      "\tTrain Loss: 0.646 | Train PPL:   1.907\n",
      "\t Val. Loss: 2.772 |  Val. PPL:  15.987 | Val. Rouge:   0.058\n",
      "Epoch: 146 | Time: 0m 6s\n",
      "\tTrain Loss: 0.629 | Train PPL:   1.876\n",
      "\t Val. Loss: 2.764 |  Val. PPL:  15.859 | Val. Rouge:   0.057\n",
      "Epoch: 147 | Time: 0m 6s\n",
      "\tTrain Loss: 0.631 | Train PPL:   1.879\n",
      "\t Val. Loss: 2.792 |  Val. PPL:  16.311 | Val. Rouge:   0.057\n",
      "Epoch: 148 | Time: 0m 6s\n",
      "\tTrain Loss: 0.615 | Train PPL:   1.851\n",
      "\t Val. Loss: 2.788 |  Val. PPL:  16.241 | Val. Rouge:   0.058\n",
      "Epoch: 149 | Time: 0m 6s\n",
      "\tTrain Loss: 0.616 | Train PPL:   1.852\n",
      "\t Val. Loss: 2.809 |  Val. PPL:  16.593 | Val. Rouge:   0.055\n",
      "Epoch: 150 | Time: 0m 6s\n",
      "\tTrain Loss: 0.612 | Train PPL:   1.844\n",
      "\t Val. Loss: 2.792 |  Val. PPL:  16.309 | Val. Rouge:   0.053\n",
      "Epoch: 151 | Time: 0m 6s\n",
      "\tTrain Loss: 0.608 | Train PPL:   1.837\n",
      "\t Val. Loss: 2.798 |  Val. PPL:  16.408 | Val. Rouge:   0.056\n",
      "Epoch: 152 | Time: 0m 6s\n",
      "\tTrain Loss: 0.605 | Train PPL:   1.831\n",
      "\t Val. Loss: 2.798 |  Val. PPL:  16.415 | Val. Rouge:   0.058\n",
      "Epoch: 153 | Time: 0m 7s\n",
      "\tTrain Loss: 0.601 | Train PPL:   1.823\n",
      "\t Val. Loss: 2.813 |  Val. PPL:  16.660 | Val. Rouge:   0.056\n",
      "Epoch: 154 | Time: 0m 7s\n",
      "\tTrain Loss: 0.604 | Train PPL:   1.830\n",
      "\t Val. Loss: 2.818 |  Val. PPL:  16.751 | Val. Rouge:   0.056\n",
      "Epoch: 155 | Time: 0m 7s\n",
      "\tTrain Loss: 0.595 | Train PPL:   1.813\n",
      "\t Val. Loss: 2.816 |  Val. PPL:  16.706 | Val. Rouge:   0.055\n",
      "Epoch: 156 | Time: 0m 7s\n",
      "\tTrain Loss: 0.581 | Train PPL:   1.789\n",
      "\t Val. Loss: 2.817 |  Val. PPL:  16.723 | Val. Rouge:   0.057\n",
      "Epoch: 157 | Time: 0m 6s\n",
      "\tTrain Loss: 0.572 | Train PPL:   1.772\n",
      "\t Val. Loss: 2.842 |  Val. PPL:  17.153 | Val. Rouge:   0.055\n",
      "Epoch: 158 | Time: 0m 6s\n",
      "\tTrain Loss: 0.574 | Train PPL:   1.776\n",
      "\t Val. Loss: 2.849 |  Val. PPL:  17.265 | Val. Rouge:   0.056\n",
      "Epoch: 159 | Time: 0m 6s\n",
      "\tTrain Loss: 0.571 | Train PPL:   1.769\n",
      "\t Val. Loss: 2.846 |  Val. PPL:  17.219 | Val. Rouge:   0.056\n",
      "Epoch: 160 | Time: 0m 6s\n",
      "\tTrain Loss: 0.566 | Train PPL:   1.761\n",
      "\t Val. Loss: 2.845 |  Val. PPL:  17.206 | Val. Rouge:   0.063\n",
      "Epoch: 161 | Time: 0m 6s\n",
      "\tTrain Loss: 0.565 | Train PPL:   1.760\n",
      "\t Val. Loss: 2.887 |  Val. PPL:  17.944 | Val. Rouge:   0.057\n",
      "Epoch: 162 | Time: 0m 6s\n",
      "\tTrain Loss: 0.553 | Train PPL:   1.738\n",
      "\t Val. Loss: 2.870 |  Val. PPL:  17.638 | Val. Rouge:   0.060\n",
      "Epoch: 163 | Time: 0m 6s\n",
      "\tTrain Loss: 0.545 | Train PPL:   1.724\n",
      "\t Val. Loss: 2.866 |  Val. PPL:  17.559 | Val. Rouge:   0.059\n",
      "Epoch: 164 | Time: 0m 6s\n",
      "\tTrain Loss: 0.547 | Train PPL:   1.729\n",
      "\t Val. Loss: 2.882 |  Val. PPL:  17.845 | Val. Rouge:   0.058\n",
      "Epoch: 165 | Time: 0m 6s\n",
      "\tTrain Loss: 0.540 | Train PPL:   1.716\n",
      "\t Val. Loss: 2.868 |  Val. PPL:  17.607 | Val. Rouge:   0.059\n",
      "Epoch: 166 | Time: 0m 6s\n",
      "\tTrain Loss: 0.541 | Train PPL:   1.718\n",
      "\t Val. Loss: 2.893 |  Val. PPL:  18.041 | Val. Rouge:   0.060\n",
      "Epoch: 167 | Time: 0m 6s\n",
      "\tTrain Loss: 0.533 | Train PPL:   1.705\n",
      "\t Val. Loss: 2.906 |  Val. PPL:  18.282 | Val. Rouge:   0.060\n",
      "Epoch: 168 | Time: 0m 6s\n",
      "\tTrain Loss: 0.531 | Train PPL:   1.701\n",
      "\t Val. Loss: 2.896 |  Val. PPL:  18.111 | Val. Rouge:   0.061\n",
      "Epoch: 169 | Time: 0m 6s\n",
      "\tTrain Loss: 0.524 | Train PPL:   1.689\n",
      "\t Val. Loss: 2.906 |  Val. PPL:  18.288 | Val. Rouge:   0.056\n",
      "Epoch: 170 | Time: 0m 6s\n",
      "\tTrain Loss: 0.511 | Train PPL:   1.666\n",
      "\t Val. Loss: 2.911 |  Val. PPL:  18.369 | Val. Rouge:   0.058\n",
      "Epoch: 171 | Time: 0m 7s\n",
      "\tTrain Loss: 0.516 | Train PPL:   1.675\n",
      "\t Val. Loss: 2.916 |  Val. PPL:  18.476 | Val. Rouge:   0.060\n",
      "Epoch: 172 | Time: 0m 7s\n",
      "\tTrain Loss: 0.503 | Train PPL:   1.654\n",
      "\t Val. Loss: 2.927 |  Val. PPL:  18.674 | Val. Rouge:   0.059\n",
      "Epoch: 173 | Time: 0m 6s\n",
      "\tTrain Loss: 0.506 | Train PPL:   1.658\n",
      "\t Val. Loss: 2.925 |  Val. PPL:  18.629 | Val. Rouge:   0.059\n",
      "Epoch: 174 | Time: 0m 6s\n",
      "\tTrain Loss: 0.504 | Train PPL:   1.656\n",
      "\t Val. Loss: 2.949 |  Val. PPL:  19.088 | Val. Rouge:   0.060\n",
      "Epoch: 175 | Time: 0m 6s\n",
      "\tTrain Loss: 0.499 | Train PPL:   1.646\n",
      "\t Val. Loss: 2.938 |  Val. PPL:  18.884 | Val. Rouge:   0.059\n",
      "Epoch: 176 | Time: 0m 7s\n",
      "\tTrain Loss: 0.499 | Train PPL:   1.648\n",
      "\t Val. Loss: 2.947 |  Val. PPL:  19.049 | Val. Rouge:   0.057\n",
      "Epoch: 177 | Time: 0m 7s\n",
      "\tTrain Loss: 0.490 | Train PPL:   1.633\n",
      "\t Val. Loss: 2.941 |  Val. PPL:  18.932 | Val. Rouge:   0.058\n",
      "Epoch: 178 | Time: 0m 6s\n",
      "\tTrain Loss: 0.487 | Train PPL:   1.628\n",
      "\t Val. Loss: 2.947 |  Val. PPL:  19.053 | Val. Rouge:   0.059\n",
      "Epoch: 179 | Time: 0m 6s\n",
      "\tTrain Loss: 0.482 | Train PPL:   1.620\n",
      "\t Val. Loss: 2.951 |  Val. PPL:  19.121 | Val. Rouge:   0.057\n",
      "Epoch: 180 | Time: 0m 7s\n",
      "\tTrain Loss: 0.477 | Train PPL:   1.611\n",
      "\t Val. Loss: 2.951 |  Val. PPL:  19.131 | Val. Rouge:   0.058\n",
      "Epoch: 181 | Time: 0m 7s\n",
      "\tTrain Loss: 0.490 | Train PPL:   1.632\n",
      "\t Val. Loss: 2.971 |  Val. PPL:  19.503 | Val. Rouge:   0.056\n",
      "Epoch: 182 | Time: 0m 7s\n",
      "\tTrain Loss: 0.472 | Train PPL:   1.604\n",
      "\t Val. Loss: 2.972 |  Val. PPL:  19.537 | Val. Rouge:   0.057\n",
      "Epoch: 183 | Time: 0m 6s\n",
      "\tTrain Loss: 0.480 | Train PPL:   1.616\n",
      "\t Val. Loss: 2.990 |  Val. PPL:  19.885 | Val. Rouge:   0.054\n",
      "Epoch: 184 | Time: 0m 7s\n",
      "\tTrain Loss: 0.464 | Train PPL:   1.590\n",
      "\t Val. Loss: 2.991 |  Val. PPL:  19.910 | Val. Rouge:   0.062\n",
      "Epoch: 185 | Time: 0m 7s\n",
      "\tTrain Loss: 0.456 | Train PPL:   1.578\n",
      "\t Val. Loss: 2.991 |  Val. PPL:  19.897 | Val. Rouge:   0.057\n",
      "Epoch: 186 | Time: 0m 7s\n",
      "\tTrain Loss: 0.451 | Train PPL:   1.570\n",
      "\t Val. Loss: 2.996 |  Val. PPL:  19.996 | Val. Rouge:   0.057\n",
      "Epoch: 187 | Time: 0m 7s\n",
      "\tTrain Loss: 0.456 | Train PPL:   1.577\n",
      "\t Val. Loss: 2.992 |  Val. PPL:  19.919 | Val. Rouge:   0.056\n",
      "Epoch: 188 | Time: 0m 7s\n",
      "\tTrain Loss: 0.446 | Train PPL:   1.563\n",
      "\t Val. Loss: 2.985 |  Val. PPL:  19.783 | Val. Rouge:   0.057\n",
      "Epoch: 189 | Time: 0m 7s\n",
      "\tTrain Loss: 0.442 | Train PPL:   1.557\n",
      "\t Val. Loss: 3.005 |  Val. PPL:  20.195 | Val. Rouge:   0.058\n",
      "Epoch: 190 | Time: 0m 7s\n",
      "\tTrain Loss: 0.452 | Train PPL:   1.572\n",
      "\t Val. Loss: 3.014 |  Val. PPL:  20.368 | Val. Rouge:   0.062\n",
      "Epoch: 191 | Time: 0m 6s\n",
      "\tTrain Loss: 0.443 | Train PPL:   1.557\n",
      "\t Val. Loss: 3.020 |  Val. PPL:  20.489 | Val. Rouge:   0.061\n",
      "Epoch: 192 | Time: 0m 6s\n",
      "\tTrain Loss: 0.439 | Train PPL:   1.552\n",
      "\t Val. Loss: 3.033 |  Val. PPL:  20.768 | Val. Rouge:   0.060\n",
      "Epoch: 193 | Time: 0m 7s\n",
      "\tTrain Loss: 0.433 | Train PPL:   1.542\n",
      "\t Val. Loss: 3.026 |  Val. PPL:  20.620 | Val. Rouge:   0.060\n",
      "Epoch: 194 | Time: 0m 6s\n",
      "\tTrain Loss: 0.434 | Train PPL:   1.543\n",
      "\t Val. Loss: 3.032 |  Val. PPL:  20.737 | Val. Rouge:   0.054\n",
      "Epoch: 195 | Time: 0m 6s\n",
      "\tTrain Loss: 0.420 | Train PPL:   1.521\n",
      "\t Val. Loss: 3.042 |  Val. PPL:  20.944 | Val. Rouge:   0.059\n",
      "Epoch: 196 | Time: 0m 7s\n",
      "\tTrain Loss: 0.430 | Train PPL:   1.538\n",
      "\t Val. Loss: 3.039 |  Val. PPL:  20.881 | Val. Rouge:   0.060\n",
      "Epoch: 197 | Time: 0m 6s\n",
      "\tTrain Loss: 0.418 | Train PPL:   1.518\n",
      "\t Val. Loss: 3.047 |  Val. PPL:  21.048 | Val. Rouge:   0.063\n",
      "Epoch: 198 | Time: 0m 6s\n",
      "\tTrain Loss: 0.411 | Train PPL:   1.508\n",
      "\t Val. Loss: 3.061 |  Val. PPL:  21.346 | Val. Rouge:   0.061\n",
      "Epoch: 199 | Time: 0m 6s\n",
      "\tTrain Loss: 0.413 | Train PPL:   1.511\n",
      "\t Val. Loss: 3.056 |  Val. PPL:  21.246 | Val. Rouge:   0.060\n",
      "Epoch: 200 | Time: 0m 7s\n",
      "\tTrain Loss: 0.411 | Train PPL:   1.508\n",
      "\t Val. Loss: 3.062 |  Val. PPL:  21.374 | Val. Rouge:   0.066\n",
      "Epoch: 201 | Time: 0m 6s\n",
      "\tTrain Loss: 0.410 | Train PPL:   1.507\n",
      "\t Val. Loss: 3.093 |  Val. PPL:  22.053 | Val. Rouge:   0.059\n",
      "Epoch: 202 | Time: 0m 6s\n",
      "\tTrain Loss: 0.408 | Train PPL:   1.504\n",
      "\t Val. Loss: 3.077 |  Val. PPL:  21.683 | Val. Rouge:   0.061\n",
      "Epoch: 203 | Time: 0m 6s\n",
      "\tTrain Loss: 0.401 | Train PPL:   1.493\n",
      "\t Val. Loss: 3.081 |  Val. PPL:  21.774 | Val. Rouge:   0.061\n",
      "Epoch: 204 | Time: 0m 7s\n",
      "\tTrain Loss: 0.396 | Train PPL:   1.487\n",
      "\t Val. Loss: 3.067 |  Val. PPL:  21.487 | Val. Rouge:   0.058\n",
      "Epoch: 205 | Time: 0m 6s\n",
      "\tTrain Loss: 0.395 | Train PPL:   1.484\n",
      "\t Val. Loss: 3.091 |  Val. PPL:  22.002 | Val. Rouge:   0.063\n",
      "Epoch: 206 | Time: 0m 6s\n",
      "\tTrain Loss: 0.395 | Train PPL:   1.484\n",
      "\t Val. Loss: 3.094 |  Val. PPL:  22.073 | Val. Rouge:   0.064\n",
      "Epoch: 207 | Time: 0m 7s\n",
      "\tTrain Loss: 0.386 | Train PPL:   1.471\n",
      "\t Val. Loss: 3.093 |  Val. PPL:  22.037 | Val. Rouge:   0.064\n",
      "Epoch: 208 | Time: 0m 7s\n",
      "\tTrain Loss: 0.387 | Train PPL:   1.472\n",
      "\t Val. Loss: 3.105 |  Val. PPL:  22.305 | Val. Rouge:   0.064\n",
      "Epoch: 209 | Time: 0m 7s\n",
      "\tTrain Loss: 0.384 | Train PPL:   1.468\n",
      "\t Val. Loss: 3.127 |  Val. PPL:  22.806 | Val. Rouge:   0.064\n",
      "Epoch: 210 | Time: 0m 7s\n",
      "\tTrain Loss: 0.398 | Train PPL:   1.489\n",
      "\t Val. Loss: 3.123 |  Val. PPL:  22.718 | Val. Rouge:   0.060\n",
      "Epoch: 211 | Time: 0m 7s\n",
      "\tTrain Loss: 0.382 | Train PPL:   1.465\n",
      "\t Val. Loss: 3.142 |  Val. PPL:  23.159 | Val. Rouge:   0.061\n",
      "Epoch: 212 | Time: 0m 6s\n",
      "\tTrain Loss: 0.376 | Train PPL:   1.456\n",
      "\t Val. Loss: 3.122 |  Val. PPL:  22.680 | Val. Rouge:   0.060\n",
      "Epoch: 213 | Time: 0m 6s\n",
      "\tTrain Loss: 0.376 | Train PPL:   1.456\n",
      "\t Val. Loss: 3.141 |  Val. PPL:  23.130 | Val. Rouge:   0.062\n",
      "Epoch: 214 | Time: 0m 7s\n",
      "\tTrain Loss: 0.368 | Train PPL:   1.445\n",
      "\t Val. Loss: 3.143 |  Val. PPL:  23.165 | Val. Rouge:   0.059\n",
      "Epoch: 215 | Time: 0m 6s\n",
      "\tTrain Loss: 0.370 | Train PPL:   1.448\n",
      "\t Val. Loss: 3.141 |  Val. PPL:  23.128 | Val. Rouge:   0.062\n",
      "Epoch: 216 | Time: 0m 6s\n",
      "\tTrain Loss: 0.360 | Train PPL:   1.433\n",
      "\t Val. Loss: 3.144 |  Val. PPL:  23.200 | Val. Rouge:   0.062\n",
      "Epoch: 217 | Time: 0m 7s\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.435\n",
      "\t Val. Loss: 3.153 |  Val. PPL:  23.401 | Val. Rouge:   0.062\n",
      "Epoch: 218 | Time: 0m 6s\n",
      "\tTrain Loss: 0.362 | Train PPL:   1.436\n",
      "\t Val. Loss: 3.171 |  Val. PPL:  23.819 | Val. Rouge:   0.064\n",
      "Epoch: 219 | Time: 0m 6s\n",
      "\tTrain Loss: 0.357 | Train PPL:   1.429\n",
      "\t Val. Loss: 3.173 |  Val. PPL:  23.879 | Val. Rouge:   0.058\n",
      "Epoch: 220 | Time: 0m 6s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.417\n",
      "\t Val. Loss: 3.171 |  Val. PPL:  23.834 | Val. Rouge:   0.061\n",
      "Epoch: 221 | Time: 0m 6s\n",
      "\tTrain Loss: 0.356 | Train PPL:   1.427\n",
      "\t Val. Loss: 3.172 |  Val. PPL:  23.859 | Val. Rouge:   0.062\n",
      "Epoch: 222 | Time: 0m 6s\n",
      "\tTrain Loss: 0.354 | Train PPL:   1.425\n",
      "\t Val. Loss: 3.164 |  Val. PPL:  23.672 | Val. Rouge:   0.063\n",
      "Epoch: 223 | Time: 0m 6s\n",
      "\tTrain Loss: 0.346 | Train PPL:   1.414\n",
      "\t Val. Loss: 3.199 |  Val. PPL:  24.514 | Val. Rouge:   0.063\n",
      "Epoch: 224 | Time: 0m 6s\n",
      "\tTrain Loss: 0.353 | Train PPL:   1.423\n",
      "\t Val. Loss: 3.177 |  Val. PPL:  23.974 | Val. Rouge:   0.066\n",
      "Epoch: 225 | Time: 0m 6s\n",
      "\tTrain Loss: 0.342 | Train PPL:   1.408\n",
      "\t Val. Loss: 3.206 |  Val. PPL:  24.669 | Val. Rouge:   0.064\n",
      "Epoch: 226 | Time: 0m 6s\n",
      "\tTrain Loss: 0.343 | Train PPL:   1.410\n",
      "\t Val. Loss: 3.219 |  Val. PPL:  25.010 | Val. Rouge:   0.064\n",
      "Epoch: 227 | Time: 0m 6s\n",
      "\tTrain Loss: 0.348 | Train PPL:   1.417\n",
      "\t Val. Loss: 3.206 |  Val. PPL:  24.689 | Val. Rouge:   0.060\n",
      "Epoch: 228 | Time: 0m 6s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.410\n",
      "\t Val. Loss: 3.220 |  Val. PPL:  25.028 | Val. Rouge:   0.065\n",
      "Epoch: 229 | Time: 0m 6s\n",
      "\tTrain Loss: 0.331 | Train PPL:   1.392\n",
      "\t Val. Loss: 3.222 |  Val. PPL:  25.087 | Val. Rouge:   0.058\n",
      "Epoch: 230 | Time: 0m 6s\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.381\n",
      "\t Val. Loss: 3.229 |  Val. PPL:  25.244 | Val. Rouge:   0.061\n",
      "Epoch: 231 | Time: 0m 6s\n",
      "\tTrain Loss: 0.329 | Train PPL:   1.390\n",
      "\t Val. Loss: 3.246 |  Val. PPL:  25.691 | Val. Rouge:   0.059\n",
      "Epoch: 232 | Time: 0m 6s\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.384\n",
      "\t Val. Loss: 3.232 |  Val. PPL:  25.331 | Val. Rouge:   0.062\n",
      "Epoch: 233 | Time: 0m 6s\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 3.232 |  Val. PPL:  25.320 | Val. Rouge:   0.064\n",
      "Epoch: 234 | Time: 0m 6s\n",
      "\tTrain Loss: 0.319 | Train PPL:   1.376\n",
      "\t Val. Loss: 3.244 |  Val. PPL:  25.644 | Val. Rouge:   0.064\n",
      "Epoch: 235 | Time: 0m 6s\n",
      "\tTrain Loss: 0.317 | Train PPL:   1.373\n",
      "\t Val. Loss: 3.231 |  Val. PPL:  25.294 | Val. Rouge:   0.061\n",
      "Epoch: 236 | Time: 0m 6s\n",
      "\tTrain Loss: 0.317 | Train PPL:   1.373\n",
      "\t Val. Loss: 3.254 |  Val. PPL:  25.897 | Val. Rouge:   0.064\n",
      "Epoch: 237 | Time: 0m 6s\n",
      "\tTrain Loss: 0.313 | Train PPL:   1.368\n",
      "\t Val. Loss: 3.254 |  Val. PPL:  25.900 | Val. Rouge:   0.062\n",
      "Epoch: 238 | Time: 0m 6s\n",
      "\tTrain Loss: 0.316 | Train PPL:   1.372\n",
      "\t Val. Loss: 3.268 |  Val. PPL:  26.266 | Val. Rouge:   0.067\n",
      "Epoch: 239 | Time: 0m 6s\n",
      "\tTrain Loss: 0.313 | Train PPL:   1.367\n",
      "\t Val. Loss: 3.274 |  Val. PPL:  26.425 | Val. Rouge:   0.065\n",
      "Epoch: 240 | Time: 0m 6s\n",
      "\tTrain Loss: 0.310 | Train PPL:   1.363\n",
      "\t Val. Loss: 3.288 |  Val. PPL:  26.784 | Val. Rouge:   0.064\n",
      "Epoch: 241 | Time: 0m 6s\n",
      "\tTrain Loss: 0.310 | Train PPL:   1.364\n",
      "\t Val. Loss: 3.295 |  Val. PPL:  26.964 | Val. Rouge:   0.060\n",
      "Epoch: 242 | Time: 0m 6s\n",
      "\tTrain Loss: 0.313 | Train PPL:   1.367\n",
      "\t Val. Loss: 3.282 |  Val. PPL:  26.639 | Val. Rouge:   0.063\n",
      "Epoch: 243 | Time: 0m 6s\n",
      "\tTrain Loss: 0.308 | Train PPL:   1.361\n",
      "\t Val. Loss: 3.287 |  Val. PPL:  26.758 | Val. Rouge:   0.064\n",
      "Epoch: 244 | Time: 0m 6s\n",
      "\tTrain Loss: 0.299 | Train PPL:   1.348\n",
      "\t Val. Loss: 3.287 |  Val. PPL:  26.750 | Val. Rouge:   0.064\n",
      "Epoch: 245 | Time: 0m 6s\n",
      "\tTrain Loss: 0.299 | Train PPL:   1.349\n",
      "\t Val. Loss: 3.296 |  Val. PPL:  27.005 | Val. Rouge:   0.059\n",
      "Epoch: 246 | Time: 0m 6s\n",
      "\tTrain Loss: 0.301 | Train PPL:   1.351\n",
      "\t Val. Loss: 3.305 |  Val. PPL:  27.248 | Val. Rouge:   0.060\n",
      "Epoch: 247 | Time: 0m 6s\n",
      "\tTrain Loss: 0.295 | Train PPL:   1.343\n",
      "\t Val. Loss: 3.313 |  Val. PPL:  27.469 | Val. Rouge:   0.064\n",
      "Epoch: 248 | Time: 0m 6s\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.338\n",
      "\t Val. Loss: 3.318 |  Val. PPL:  27.611 | Val. Rouge:   0.062\n",
      "Epoch: 249 | Time: 0m 6s\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.329\n",
      "\t Val. Loss: 3.325 |  Val. PPL:  27.800 | Val. Rouge:   0.063\n",
      "Epoch: 250 | Time: 0m 6s\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.338\n",
      "\t Val. Loss: 3.334 |  Val. PPL:  28.038 | Val. Rouge:   0.056\n",
      "Epoch: 251 | Time: 0m 6s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 3.332 |  Val. PPL:  27.999 | Val. Rouge:   0.061\n",
      "Epoch: 252 | Time: 0m 6s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.332\n",
      "\t Val. Loss: 3.339 |  Val. PPL:  28.186 | Val. Rouge:   0.062\n",
      "Epoch: 253 | Time: 0m 6s\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.337\n",
      "\t Val. Loss: 3.350 |  Val. PPL:  28.500 | Val. Rouge:   0.061\n",
      "Epoch: 254 | Time: 0m 7s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 3.343 |  Val. PPL:  28.306 | Val. Rouge:   0.056\n",
      "Epoch: 255 | Time: 0m 6s\n",
      "\tTrain Loss: 0.281 | Train PPL:   1.325\n",
      "\t Val. Loss: 3.343 |  Val. PPL:  28.300 | Val. Rouge:   0.057\n",
      "Epoch: 256 | Time: 0m 7s\n",
      "\tTrain Loss: 0.276 | Train PPL:   1.318\n",
      "\t Val. Loss: 3.340 |  Val. PPL:  28.212 | Val. Rouge:   0.066\n",
      "Epoch: 257 | Time: 0m 6s\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.312\n",
      "\t Val. Loss: 3.338 |  Val. PPL:  28.177 | Val. Rouge:   0.060\n",
      "Epoch: 258 | Time: 0m 6s\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.317\n",
      "\t Val. Loss: 3.346 |  Val. PPL:  28.387 | Val. Rouge:   0.063\n",
      "Epoch: 259 | Time: 0m 7s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 3.360 |  Val. PPL:  28.782 | Val. Rouge:   0.066\n",
      "Epoch: 260 | Time: 0m 7s\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 3.373 |  Val. PPL:  29.160 | Val. Rouge:   0.064\n",
      "Epoch: 261 | Time: 0m 7s\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 3.382 |  Val. PPL:  29.416 | Val. Rouge:   0.061\n",
      "Epoch: 262 | Time: 0m 7s\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 3.368 |  Val. PPL:  29.010 | Val. Rouge:   0.064\n",
      "Epoch: 263 | Time: 0m 7s\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 3.379 |  Val. PPL:  29.354 | Val. Rouge:   0.061\n",
      "Epoch: 264 | Time: 0m 7s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.303\n",
      "\t Val. Loss: 3.392 |  Val. PPL:  29.733 | Val. Rouge:   0.062\n",
      "Epoch: 265 | Time: 0m 7s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.299\n",
      "\t Val. Loss: 3.385 |  Val. PPL:  29.521 | Val. Rouge:   0.060\n",
      "Epoch: 266 | Time: 0m 7s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.304\n",
      "\t Val. Loss: 3.386 |  Val. PPL:  29.551 | Val. Rouge:   0.062\n",
      "Epoch: 267 | Time: 0m 7s\n",
      "\tTrain Loss: 0.257 | Train PPL:   1.293\n",
      "\t Val. Loss: 3.400 |  Val. PPL:  29.952 | Val. Rouge:   0.056\n",
      "Epoch: 268 | Time: 0m 7s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 3.397 |  Val. PPL:  29.871 | Val. Rouge:   0.060\n",
      "Epoch: 269 | Time: 0m 7s\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.290\n",
      "\t Val. Loss: 3.417 |  Val. PPL:  30.489 | Val. Rouge:   0.061\n",
      "Epoch: 270 | Time: 0m 6s\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.285\n",
      "\t Val. Loss: 3.411 |  Val. PPL:  30.288 | Val. Rouge:   0.060\n",
      "Epoch: 271 | Time: 0m 6s\n",
      "\tTrain Loss: 0.257 | Train PPL:   1.293\n",
      "\t Val. Loss: 3.434 |  Val. PPL:  31.010 | Val. Rouge:   0.061\n",
      "Epoch: 272 | Time: 0m 6s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 3.427 |  Val. PPL:  30.798 | Val. Rouge:   0.060\n",
      "Epoch: 273 | Time: 0m 6s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 3.421 |  Val. PPL:  30.592 | Val. Rouge:   0.065\n",
      "Epoch: 274 | Time: 0m 7s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 3.431 |  Val. PPL:  30.907 | Val. Rouge:   0.060\n",
      "Epoch: 275 | Time: 0m 7s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
      "\t Val. Loss: 3.447 |  Val. PPL:  31.419 | Val. Rouge:   0.058\n",
      "Epoch: 276 | Time: 0m 7s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
      "\t Val. Loss: 3.450 |  Val. PPL:  31.491 | Val. Rouge:   0.062\n",
      "Epoch: 277 | Time: 0m 6s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
      "\t Val. Loss: 3.455 |  Val. PPL:  31.654 | Val. Rouge:   0.063\n",
      "Epoch: 278 | Time: 0m 7s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.273\n",
      "\t Val. Loss: 3.442 |  Val. PPL:  31.241 | Val. Rouge:   0.064\n",
      "Epoch: 279 | Time: 0m 7s\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.269\n",
      "\t Val. Loss: 3.435 |  Val. PPL:  31.036 | Val. Rouge:   0.061\n",
      "Epoch: 280 | Time: 0m 6s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.273\n",
      "\t Val. Loss: 3.441 |  Val. PPL:  31.229 | Val. Rouge:   0.061\n",
      "Epoch: 281 | Time: 0m 6s\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.263\n",
      "\t Val. Loss: 3.472 |  Val. PPL:  32.209 | Val. Rouge:   0.056\n",
      "Epoch: 282 | Time: 0m 6s\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.264\n",
      "\t Val. Loss: 3.476 |  Val. PPL:  32.336 | Val. Rouge:   0.061\n",
      "Epoch: 283 | Time: 0m 6s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 3.478 |  Val. PPL:  32.381 | Val. Rouge:   0.061\n",
      "Epoch: 284 | Time: 0m 7s\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.253\n",
      "\t Val. Loss: 3.470 |  Val. PPL:  32.139 | Val. Rouge:   0.062\n",
      "Epoch: 285 | Time: 0m 7s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
      "\t Val. Loss: 3.472 |  Val. PPL:  32.196 | Val. Rouge:   0.061\n",
      "Epoch: 286 | Time: 0m 6s\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.263\n",
      "\t Val. Loss: 3.492 |  Val. PPL:  32.861 | Val. Rouge:   0.063\n",
      "Epoch: 287 | Time: 0m 7s\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
      "\t Val. Loss: 3.507 |  Val. PPL:  33.363 | Val. Rouge:   0.059\n",
      "Epoch: 288 | Time: 0m 7s\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
      "\t Val. Loss: 3.503 |  Val. PPL:  33.222 | Val. Rouge:   0.060\n",
      "Epoch: 289 | Time: 0m 7s\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 3.498 |  Val. PPL:  33.040 | Val. Rouge:   0.061\n",
      "Epoch: 290 | Time: 0m 7s\n",
      "\tTrain Loss: 0.226 | Train PPL:   1.254\n",
      "\t Val. Loss: 3.516 |  Val. PPL:  33.642 | Val. Rouge:   0.066\n",
      "Epoch: 291 | Time: 0m 6s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 3.505 |  Val. PPL:  33.292 | Val. Rouge:   0.063\n",
      "Epoch: 292 | Time: 0m 7s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 3.523 |  Val. PPL:  33.882 | Val. Rouge:   0.056\n",
      "Epoch: 293 | Time: 0m 6s\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 3.544 |  Val. PPL:  34.598 | Val. Rouge:   0.058\n",
      "Epoch: 294 | Time: 0m 6s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 3.536 |  Val. PPL:  34.318 | Val. Rouge:   0.060\n",
      "Epoch: 295 | Time: 0m 7s\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 3.545 |  Val. PPL:  34.653 | Val. Rouge:   0.059\n",
      "Epoch: 296 | Time: 0m 7s\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.244\n",
      "\t Val. Loss: 3.562 |  Val. PPL:  35.227 | Val. Rouge:   0.059\n",
      "Epoch: 297 | Time: 0m 7s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 3.550 |  Val. PPL:  34.812 | Val. Rouge:   0.055\n",
      "Epoch: 298 | Time: 0m 7s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 3.536 |  Val. PPL:  34.340 | Val. Rouge:   0.060\n",
      "Epoch: 299 | Time: 0m 6s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 3.558 |  Val. PPL:  35.109 | Val. Rouge:   0.066\n",
      "Epoch: 300 | Time: 0m 7s\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 3.585 |  Val. PPL:  36.041 | Val. Rouge:   0.063\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 300\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "best_train_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss, rouge_score = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'conala_plus_original_data.pt')\n",
    "    \n",
    "    if train_loss < best_train_loss:\n",
    "        best_train_loss = train_loss\n",
    "        torch.save(model.state_dict(), 'best_train_loss.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f} | Val. Rouge: {rouge_score:7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (tok_embedding): Embedding(1491, 256)\n",
       "    (pos_embedding): PositionalEncodingComponent(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadedAttentionComponent(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForwardComponent(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadedAttentionComponent(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForwardComponent(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadedAttentionComponent(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForwardComponent(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (tok_embedding): Embedding(1730, 256)\n",
       "    (pos_embedding): PositionalEncodingComponent(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadedAttentionComponent(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadedAttentionComponent(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForwardComponent(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadedAttentionComponent(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadedAttentionComponent(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForwardComponent(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadedAttentionComponent(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadedAttentionComponent(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForwardComponent(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc_out): Linear(in_features=256, out_features=1730, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"SRC_stio_local\",\"rb\") as f:\n",
    "  stoi = pickle.load(f)\n",
    "with open(\"TRG_itos_local\",\"rb\") as f:\n",
    "  itos = pickle.load(f)\n",
    "\n",
    "# Load model\n",
    "# trained_model = 'conala_plus_original_data.pt'\n",
    "trained_model = './best_train_loss.pt'\n",
    "model.load_state_dict(torch.load(trained_model));\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "def encode_inputs(input,vocab):\n",
    "  tokenized_input_ = [tok for tok in tokenize(input, 'hi')]\n",
    "  tokenized_input = [''] + tokenized_input_ +['']\n",
    "\n",
    "  numericalized_input = [vocab[i] for i in tokenized_input]\n",
    "\n",
    "  tensor_input = torch.LongTensor([numericalized_input])\n",
    "  \n",
    "  return tensor_input,tokenized_input_\n",
    "\n",
    "def decode_outputs(output,vocab):\n",
    "  # output: [1,1,hid_dim]\n",
    "  predicted_token = output.argmax(-1)\n",
    "  return vocab[predicted_token.item()], predicted_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_attention(sentence, translation, attention, n_heads = 8, n_rows = 4, n_cols = 2):\n",
    "    \n",
    "    assert n_rows * n_cols == n_heads\n",
    "    \n",
    "    fig = plt.figure(figsize=(15,25))\n",
    "    \n",
    "    for i in range(n_heads):\n",
    "        \n",
    "        ax = fig.add_subplot(n_rows, n_cols, i+1)\n",
    "        \n",
    "        _attention = attention.squeeze(0)[i].cpu().detach().numpy()\n",
    "\n",
    "        cax = ax.matshow(_attention, cmap='bone')\n",
    "\n",
    "        ax.tick_params(labelsize=12)\n",
    "        ax.set_xticklabels(['']+['']+[t.lower() for t in sentence]+[''], \n",
    "                           rotation=45)\n",
    "        ax.set_yticklabels(['']+translation)\n",
    "\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variables_names_in_print(matchobj):\n",
    "  statement = matchobj.group(1)\n",
    "  statement = statement.replace(\" \",\"\")\n",
    "  return \"{\"+statement+\"}\"\n",
    "\n",
    "def print_decoder_output(decoder_outputs):\n",
    "  decoder_outputs = [i for i in decoder_outputs if i is not ''] # removing redundant empty token created by tokenizer while identation during tokenization\n",
    "  combined_output = \" \".join(decoder_outputs)\n",
    "  pruned_output = re.sub(r'\\n |\\n  |\\n   ',r'\\n',combined_output) # removing empty lines\n",
    "  pruned_output = re.sub(r'{(.*?)}',variables_names_in_print,pruned_output) # setting printing variable names inside print(f'{}') statements\n",
    "  print(pruned_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Enter q or quit to exit.\n",
      "def bmi ( height : \" M e t e r s \" , weight : \" K g s \" ) : \n",
      "\t bmi = weight / ( height ** 2 ) \n",
      "\t print ( \" Y o u r   B M I   i s :   {0}   a n d   y o u   a r e   \" . format ( bmi ) , end = ' ' ' ) \n",
      "\t if ( bmi < 16 ) : \n",
      "\t\t print ( \" s e v e r e l y   u n d e r w e i g h t . \" ) \n",
      "\t elif ( bmi >= 16 and bmi < 18.5 ) : \n",
      "\t\t print ( \" u n d e r w e i g h t . \" ) \n",
      "\t elif ( bmi >= 18.5 and bmi < 25 ) : \n",
      "\t\t print ( \" h e a l t h y . \" ) \n",
      "\t elif ( bmi < 30 ) : \n",
      "\t\t print ( \" o v e r w e i g h t . \" ) \n",
      "\t elif ( bmi >= 25 and bmi >= 30 ) : \n",
      "\t\t print ( \" s e r e r e l y   o v e r w e i g h t . \" )\n"
     ]
    }
   ],
   "source": [
    "print(\" Enter q or quit to exit.\")\n",
    "\n",
    "answer_max_len = 500\n",
    "\n",
    "while(True):\n",
    "\n",
    "  input_ = input(\"Enter text:\")\n",
    "\n",
    "  if input_=='q' or input_=='quit':\n",
    "    break\n",
    "\n",
    "  src,tokenized_input_ = encode_inputs(input_,stoi)\n",
    "  src = src.to(device)\n",
    "  # src_mask = torch.ones([1,1,1,src.shape[-1]]).to(device)\n",
    "  src_mask = model.make_src_mask(src)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    enc_src = model.encoder(src,src_mask)\n",
    "  \n",
    "  trg = ''\n",
    "  trg_indexes = [stoi[trg]]\n",
    "  # trg_mask = torch.ones([1,1,1,1]).to(device)\n",
    "\n",
    "  decoder_outputs = []\n",
    "  for i in range(answer_max_len):\n",
    "    trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "    trg_mask = model.make_trg_mask(trg_tensor)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      decoder_output,encoder_decoder_attention = model.decoder(trg_tensor,enc_src,trg_mask,src_mask)\n",
    "\n",
    "    pred_token = decoder_output.argmax(2)[:,-1].item()\n",
    "\n",
    "    if pred_token == TRG.vocab.stoi[TRG.eos_token]:\n",
    "      break\n",
    "    decoder_outputs.append(itos[pred_token])\n",
    "    trg_indexes.append(pred_token)\n",
    "\n",
    "\n",
    "  print_decoder_output(decoder_outputs)\n",
    "  # display_attention(tokenized_input_,decoder_outputs,encoder_decoder_attention,DEC_HEADS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'एक फ़ंक्शन लिखें जो height (m) और weight (kg) लेता है, BMI की गणना करता है और टिप्पणियों को प्रिंट करता है'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check_question = \"Write a Python function to remove leading zeros from an IP address\"\n",
    "# for i,j in enumerate(questions):\n",
    "#   if j == check_question:\n",
    "#     print(i)\n",
    "questions[54]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def bmi(height: \"Meters\", weight: \"Kgs\"):\\n\\tbmi = weight/(height**2)\\n\\tprint(\"Your BMI is: {0} and you are \".format(bmi), end=\\'\\')\\n\\tif ( bmi < 16):\\n\\t\\tprint(\"severely underweight.\")\\n\\telif ( bmi >= 16 and bmi < 18.5):\\n\\t\\tprint(\"underweight.\")\\n\\telif ( bmi >= 18.5 and bmi < 25):\\n\\t\\tprint(\"healthy.\")\\n\\telif ( bmi >= 25 and bmi < 30):\\n\\t\\tprint(\"overweight.\")\\n\\telif ( bmi >=30):\\n\\t\\tprint(\"severely overweight.\")\\n'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[54]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('nlpenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5370e3a0b6da721edeecb8b16b0066c9120d09f8099d399af191cd9c45f56bc8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
